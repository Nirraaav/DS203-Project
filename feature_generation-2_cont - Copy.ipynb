{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from 'features_generated.csv'\n",
    "df1 = pd.read_csv('features_generated_for_clustering.csv')\n",
    "# df2 = pd.read_csv('features_generated_known_data.csv')\n",
    "\n",
    "#concatenating known data and unknown data\n",
    "# result = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "result = df1\n",
    "\n",
    "# # Separate the features and the 'File' column\n",
    "# file_names = df1['File']  # Optional: store file names if needed separately\n",
    "# features = df1.drop(columns=['File'])  # Drop the 'File' column to keep only features\n",
    "\n",
    "\n",
    "# Separate the features and the 'File' column\n",
    "file_names = result['File']  # Optional: store file names if needed separately\n",
    "features = result.drop(columns=['File'])  # Drop the 'File' column to keep only features\n",
    "\n",
    "\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "normalizer = MinMaxScaler()\n",
    "features_scaled = normalizer.fit_transform(features_scaled)\n",
    "\n",
    "\n",
    "# If you want to store the standardized features back into a DataFrame\n",
    "features_scaled_df = pd.DataFrame(features_scaled, columns=features.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_scaled_df.shape)\n",
    "features_scaled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix_features_scaled = features_scaled_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation heat map for feature just after scaling\n",
    "plt.figure(figsize=(100,100))\n",
    "sns.heatmap(correlation_matrix_features_scaled, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap (before reomving high-correlation features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# correlation heat map for feature after rremoving high correlation features\n",
    "\n",
    "# Step 1: Compute the correlation matrix\n",
    "corr_matrix = correlation_matrix_features_scaled.abs()\n",
    "\n",
    "# Step 2: Select the upper triangle of the correlation matrix\n",
    "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Step 3: Identify and remove features with a high correlation (> 0.95)\n",
    "to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.95)]\n",
    "features_reduced_df = features_scaled_df.drop(columns=to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Generate and plot the correlation heatmap\n",
    "plt.figure(figsize=(100, 100))\n",
    "sns.heatmap(features_reduced_df.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap (with high-correlation features removed)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_reduced_df.shape)\n",
    "features_reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Calculate VIF for each feature in the DataFrame\n",
    "# vif_data = pd.DataFrame()\n",
    "# vif_data['feature'] = features_reduced_df.columns\n",
    "# vif_data['VIF'] = [variance_inflation_factor(features_reduced_df.values, i) \n",
    "#                    for i in range(features_reduced_df.shape[1])]\n",
    "\n",
    "# # Plot VIF scores for all features\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# plt.barh(vif_data['feature'], vif_data['VIF'], color=\"skyblue\")\n",
    "# plt.xlabel('VIF Score')\n",
    "# plt.ylabel('Features')\n",
    "# plt.title('VIF Scores for All Features')\n",
    "# plt.legend()\n",
    "# plt.gca().invert_yaxis()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import LocalOutlierFactor\n",
    "# from sklearn.ensemble import IsolationForest\n",
    "# from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Step 1: Initialize Models\n",
    "# lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "# isoforest = IsolationForest(contamination=0.1, random_state=42)\n",
    "\n",
    "# # Step 2: Apply LOF\n",
    "# lof_outliers = lof.fit_predict(features_reduced_df)\n",
    "# lof_scores = -lof.negative_outlier_factor_  # LOF scores, higher means more outlier-like\n",
    "\n",
    "# # Step 3: Apply Isolation Forest\n",
    "# isoforest.fit(features_reduced_df)\n",
    "# iso_outliers = isoforest.predict(features_reduced_df)\n",
    "# iso_scores = isoforest.decision_function(features_reduced_df)  # Higher means less outlier-like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Converting LOF results to 0s and 1s (1 for inliers, -1 for outliers)\n",
    "# lof_outliers = np.where(lof_outliers == 1, 0, 1)\n",
    "# iso_outliers = np.where(iso_outliers == 1, 0, 1)\n",
    "\n",
    "# # Step 4: Compare Outlier Detection Counts\n",
    "# print(f\"LOF Outliers: {np.sum(lof_outliers)}\")\n",
    "# print(f\"Isolation Forest Outliers: {np.sum(iso_outliers)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Step 5: Plot the distribution of scores\n",
    "# plt.figure(figsize=(12, 6))\n",
    "\n",
    "# # LOF Scores\n",
    "# plt.subplot(1, 2, 1)\n",
    "# sns.histplot(lof_scores, bins=30, kde=True)\n",
    "# plt.title(\"LOF Outlier Scores Distribution\")\n",
    "# plt.xlabel(\"LOF Score (Higher is more outlier-like)\")\n",
    "\n",
    "# # Isolation Forest Scores\n",
    "# plt.subplot(1, 2, 2)\n",
    "# sns.histplot(iso_scores, bins=30, kde=True)\n",
    "# plt.title(\"Isolation Forest Scores Distribution\")\n",
    "# plt.xlabel(\"Isolation Score (Lower is more outlier-like)\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Step 6: (Optional) Precision, Recall, F1 Score\n",
    "# # If you have labeled data for outliers, use these metrics to evaluate model performance:\n",
    "# # Assume `true_labels` contains 0 for inliers, 1 for actual outliers (if available)\n",
    "# # print(\"LOF Precision, Recall, F1:\", precision_score(true_labels, lof_outliers),\n",
    "# #       recall_score(true_labels, lof_outliers), f1_score(true_labels, lof_outliers))\n",
    "# # print(\"Isolation Forest Precision, Recall, F1:\", precision_score(true_labels, iso_outliers),\n",
    "# #       recall_score(true_labels, iso_outliers), f1_score(true_labels, iso_outliers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: If you want to include the 'File' column back in the standardized DataFrame\n",
    "# features_reduced_df.insert(0, 'File', file_names)\n",
    "features_reduced_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_scaled = features_reduced_df\n",
    "features_scaled_df = features_scaled_df\n",
    "\n",
    "# Feature Engineering\n",
    "# 1. Dimensionality Reduction using PCA\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of the variance\n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = explained_variance.cumsum()\n",
    "# Find the number of components that retain 95% of the variance\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1  # Add 1 because index starts from 0\n",
    "# Plot the cumulative explained variance\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance in PCA')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--')  # 95% variance threshold\n",
    "plt.show()\n",
    "print(f\"Number of components that retain 95% of the variance: {n_components_95}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Feature Selection\n",
    "selector = SelectKBest(mutual_info_classif, k=200)  # Select top k features\n",
    "features_selected = selector.fit_transform(features_scaled, np.random.randint(0, 2, size=features.shape[0]))  # Random target for selection\n",
    "\n",
    "# 3. t-SNE \n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "features_tsne = tsne.fit_transform(features_scaled)\n",
    "\n",
    "# 4. UMAP\n",
    "umap_reducer = umap.UMAP(n_components=55, random_state=42)  # For 2D, change n_components to 3 for 3D\n",
    "features_umap = umap_reducer.fit_transform(features_pca)\n",
    "\n",
    "# 5. Kernal PCA\n",
    "kpca = KernelPCA(n_components=50, kernel='rbf', gamma=0.1)\n",
    "features_kpca = kpca.fit_transform(features_scaled)\n",
    "\n",
    "# # Choosing the best feature set\n",
    "# if features_selected.shape[1] < features_pca.shape[1]:  # Choose based on lower dimensionality\n",
    "#     print(\"Using SelectKbest\")\n",
    "#     features_for_clustering = features_selected\n",
    "# else:\n",
    "#     print(\"Using PCA\")\n",
    "#     features_for_clustering = features_pca\n",
    "\n",
    "### Selecting the required features\n",
    "features_for_clustering = features_umap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate Euclidean distance matrix\n",
    "distance_matrix = squareform(pdist(features_for_clustering, metric='euclidean'))\n",
    "\n",
    "# Plot as heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(distance_matrix, cmap='viridis', cbar=True)\n",
    "plt.title(\"Euclidean Distance Matrix\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Sample Index\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_needed = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clustering\n",
    "# 1. KMeans Clustering\n",
    "kmeans = KMeans(n_clusters=clusters_needed, random_state=42)  # Adjust clusters based on data\n",
    "kmeans_labels = kmeans.fit_predict(features_for_clustering)\n",
    "\n",
    "# Evaluate KMeans\n",
    "print(\"KMeans Clustering:\")\n",
    "# Calculate clustering metrics for K-means\n",
    "silhouette_kmeans = silhouette_score(features_for_clustering, kmeans_labels)\n",
    "dbi_kmeans = davies_bouldin_score(features_for_clustering, kmeans_labels)\n",
    "ch_kmeans = calinski_harabasz_score(features_for_clustering, kmeans_labels)\n",
    "print(\"K-means Clustering Metrics:\")\n",
    "print(f\"Silhouette Score: {silhouette_kmeans:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {dbi_kmeans:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {ch_kmeans:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for linkage in ['ward', 'complete', 'average', 'single']:\n",
    "#     hierarchical = AgglomerativeClustering(n_clusters=clusters_needed, linkage=linkage)\n",
    "#     labels = hierarchical.fit_predict(features_for_clustering)\n",
    "    \n",
    "#     print(f\"Linkage: {linkage}\")\n",
    "#     print(f\"Silhouette Score: {silhouette_score(features_for_clustering, labels):.4f}\")\n",
    "#     print(f\"Davies-Bouldin Index: {davies_bouldin_score(features_for_clustering, labels):.4f}\")\n",
    "#     print(f\"Calinski-Harabasz Index: {calinski_harabasz_score(features_for_clustering, labels):.4f}\\n\")\n",
    "\n",
    "#     # Plot the Dendrogram\n",
    "#     plt.figure(figsize=(10, 7))\n",
    "#     dendrogram = sch.dendrogram(sch.linkage(features_for_clustering, method=linkage))\n",
    "\n",
    "#     # Add a horizontal line to show where the dendrogram is being cut for 6 clusters\n",
    "#     plt.axhline(y=150, color='r', linestyle='--')  # Adjust the height where the cut is made\n",
    "#     plt.title(f'Hierarchical Clustering Dendrogram: {linkage}')\n",
    "#     plt.xlabel('Sample Index')\n",
    "#     plt.ylabel('Distance')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define PCA and t-SNE transformers outside the loop to avoid redundant transformations\n",
    "pca = PCA(n_components=2)\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "# Perform PCA and t-SNE transformations\n",
    "features_pca_2d = pca.fit_transform(features_for_clustering)\n",
    "features_tsne_2d = tsne.fit_transform(features_for_clustering)\n",
    "\n",
    "# Clustering loop with additional PCA and t-SNE plots\n",
    "for linkage in ['ward', 'complete', 'average', 'single']:\n",
    "    hierarchical = AgglomerativeClustering(n_clusters=clusters_needed, linkage=linkage)\n",
    "    labels = hierarchical.fit_predict(features_for_clustering)\n",
    "    \n",
    "    print(f\"Linkage: {linkage}\")\n",
    "    print(f\"Silhouette Score: {silhouette_score(features_for_clustering, labels):.4f}\")\n",
    "    print(f\"Davies-Bouldin Index: {davies_bouldin_score(features_for_clustering, labels):.4f}\")\n",
    "    print(f\"Calinski-Harabasz Index: {calinski_harabasz_score(features_for_clustering, labels):.4f}\\n\")\n",
    "\n",
    "    # Plot the Dendrogram\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    dendrogram = sch.dendrogram(sch.linkage(features_for_clustering, method=linkage))\n",
    "    plt.axhline(y=150, color='r', linestyle='--')  # Adjust the height where the cut is made\n",
    "    plt.title(f'Hierarchical Clustering Dendrogram: {linkage}')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot 2D PCA\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.scatterplot(x=features_pca_2d[:, 0], y=features_pca_2d[:, 1], hue=labels, palette='viridis')\n",
    "    plt.title(f'2D PCA: {linkage} Linkage')\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    \n",
    "    # Plot 2D t-SNE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.scatterplot(x=features_tsne_2d[:, 0], y=features_tsne_2d[:, 1], hue=labels, palette='viridis')\n",
    "    plt.title(f'2D t-SNE: {linkage} Linkage')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Hierarchical Clustering\n",
    "hierarchical = AgglomerativeClustering(n_clusters=clusters_needed,linkage=\"complete\")\n",
    "hierarchical_labels = hierarchical.fit_predict(features_for_clustering)\n",
    "\n",
    "# Evaluate Hierarchical\n",
    "print(\"\\nHierarchical Clustering:\")\n",
    "# Calculate clustering metrics for Hierarchical Clustering\n",
    "silhouette_hierarchical = silhouette_score(features_for_clustering, hierarchical_labels)\n",
    "dbi_hierarchical = davies_bouldin_score(features_for_clustering, hierarchical_labels)\n",
    "ch_hierarchical = calinski_harabasz_score(features_for_clustering, hierarchical_labels)\n",
    "print(\"\\nHierarchical Clustering Metrics:\")\n",
    "print(f\"Silhouette Score: {silhouette_hierarchical:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {dbi_hierarchical:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {ch_hierarchical:.4f}\")\n",
    "\n",
    "# Plot the Dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram = sch.dendrogram(sch.linkage(features_for_clustering, method='complete'))\n",
    "\n",
    "# Add a horizontal line to show where the dendrogram is being cut for 6 clusters\n",
    "plt.axhline(y=150, color='r', linestyle='--')  # Adjust the height where the cut is made\n",
    "plt.title('Hierarchical Clustering Dendrogram: complete')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import DBSCAN\n",
    "\n",
    "# dbscan = DBSCAN(eps=12, min_samples=3)\n",
    "# dbscan_labels = dbscan.fit_predict(features_for_clustering)\n",
    "\n",
    "# print(\"DBSCAN metrics\")\n",
    "# print(dbscan_labels)\n",
    "\n",
    "# # Filter out noise points for metrics calculation\n",
    "# core_samples_mask = dbscan_labels != -1\n",
    "# core_features = features_for_clustering[core_samples_mask]\n",
    "# core_labels = dbscan_labels[core_samples_mask]\n",
    "\n",
    "# # Calculate metrics only on core points\n",
    "# if len(np.unique(core_labels)) > 1:  # Ensure there's more than 1 cluster\n",
    "#     silhouette = silhouette_score(core_features, core_labels)\n",
    "#     davies_bouldin = davies_bouldin_score(core_features, core_labels)\n",
    "#     calinski_harabasz = calinski_harabasz_score(core_features, core_labels)\n",
    "#     print(\"\\nDBScan Clustering Metrics:\")\n",
    "#     print(f\"Silhouette Score (core points): {silhouette:.4f}\")\n",
    "#     print(f\"Davies-Bouldin Index (core points): {davies_bouldin:.4f}\")\n",
    "#     print(f\"Calinski-Harabasz Index (core points): {calinski_harabasz:.4f}\")\n",
    "# else:\n",
    "#     print(\"Not enough clusters for metric calculation.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 3. SVC-based Clustering (using linear kernel for simplicity)\n",
    "# svc = SVC(kernel='linear')\n",
    "# svc.fit(features_for_clustering, kmeans_labels)  # Fit SVC to predict cluster labels\n",
    "# svc_labels = svc.predict(features_for_clustering)\n",
    "\n",
    "# # Evaluate SVC-based clustering\n",
    "# print(\"\\nSVC-based Clustering:\")\n",
    "# silhouette_svc = silhouette_score(features_for_clustering, svc_labels)\n",
    "# dbi_svc = davies_bouldin_score(features_for_clustering, svc_labels)\n",
    "# ch_svc = calinski_harabasz_score(features_for_clustering, svc_labels)\n",
    "# print(f\"Silhouette Score: {silhouette_score(features_for_clustering, svc_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the clustering evaluation metrics for both methods\n",
    "print(\"K-means Clustering Metrics:\")\n",
    "print(f\"Silhouette Score: {silhouette_kmeans:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {dbi_kmeans:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {ch_kmeans:.4f}\")\n",
    "\n",
    "print(\"\\nHierarchical Clustering Metrics:\")\n",
    "print(f\"Silhouette Score: {silhouette_hierarchical:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {dbi_hierarchical:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {ch_hierarchical:.4f}\")\n",
    "\n",
    "# Calculate metrics\n",
    "\n",
    "# print(\"\\nSVC Clustering Metrics:\")\n",
    "# print(f\"Silhouette Score: {silhouette_svc:.4f}\")\n",
    "# print(f\"Davies-Bouldin Index: {dbi_svc:.4f}\")\n",
    "# print(f\"Calinski-Harabasz Index: {ch_svc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display clustering results\n",
    "results_df = pd.DataFrame({\n",
    "    'File': file_names,\n",
    "    'KMeans_Label': kmeans_labels,\n",
    "    'Hierarchical_Label': hierarchical_labels,\n",
    "    # 'SVC_Label': svc_labels\n",
    "})\n",
    "\n",
    "results_df.to_csv(f'labels.csv', index=False)\n",
    "\n",
    "\n",
    "# print(\"\\nClustering Results:\")\n",
    "# print(results_df.head())\n",
    "\n",
    "# # Visualization of clustering (Optional)\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.subplot(1, 3, 1)\n",
    "# sns.scatterplot(x=features_for_clustering[:, 0], y=features_for_clustering[:, 1], hue=kmeans_labels, palette='viridis')\n",
    "# plt.title('KMeans Clustering')\n",
    "# plt.subplot(1, 3, 2)\n",
    "# sns.scatterplot(x=features_for_clustering[:, 0], y=features_for_clustering[:, 1], hue=hierarchical_labels, palette='viridis')\n",
    "# plt.title('Hierarchical Clustering')\n",
    "# plt.subplot(1, 3, 3)\n",
    "# sns.scatterplot(x=features_for_clustering[:, 0], y=features_for_clustering[:, 1], hue=svc_labels, palette='viridis')\n",
    "# plt.title('SVC-based Clustering')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a labels array kmeans_labels with labels for various files in file_names array. Now i want to change the labels in kmeans_values(integer values) corresponding to file names starting with 'Jana-Gana' as 6, starting with 'Michael-Jackson' as 7, 'Asha-Bhosle' as 8, 'Kishore-Kumar' as 8, 'Bhavgeet' as 9, 'Lavni' as 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Iterate over file_names and update kmeans_labels based on conditions\n",
    "# for idx, file_name in enumerate(file_names):\n",
    "#     if file_name.startswith('Jana-Gana'):\n",
    "#         kmeans_labels[idx] = 6\n",
    "#         hierarchical_labels[idx] = 6\n",
    "#     elif file_name.startswith('Michael-Jackson'):\n",
    "#         kmeans_labels[idx] = 7\n",
    "#         hierarchical_labels[idx] = 7\n",
    "#     elif file_name.startswith('Asha-Bhosle') or file_name.startswith('Kishore-Kumar'):\n",
    "#         kmeans_labels[idx] = 8\n",
    "#         hierarchical_labels[idx] = 8\n",
    "#     elif file_name.startswith('Bhavgeet'):\n",
    "#         kmeans_labels[idx] = 9\n",
    "#         hierarchical_labels[idx] = 9\n",
    "#     elif file_name.startswith('Lavni'):\n",
    "#         kmeans_labels[idx] = 10\n",
    "#         hierarchical_labels[idx] = 10\n",
    "\n",
    "# # kmeans_labels now has the updated labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', \n",
    "          '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf', '#ff9896']\n",
    "custom_palette = sns.color_palette(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply PCA to reduce data to 2D\n",
    "pca_2d = PCA(n_components=2, random_state=42)\n",
    "features_pca_2d = pca_2d.fit_transform(features_for_clustering)\n",
    "\n",
    "# Plot the 2D PCA visualization for KMeans and Hierarchical clustering results\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# KMeans Clustering\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(x=features_pca_2d[:, 0], y=features_pca_2d[:, 1], hue=kmeans_labels, palette='viridis')\n",
    "plt.title('KMeans Clustering (2D PCA)')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "\n",
    "# Hierarchical Clustering\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x=features_pca_2d[:, 0], y=features_pca_2d[:, 1], hue=hierarchical_labels, palette='viridis')\n",
    "plt.title('Hierarchical Clustering (2D PCA)')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D t-SNE Visualization\n",
    "tsne_2d = TSNE(n_components=2, random_state=10)\n",
    "features_tsne_2d = tsne_2d.fit_transform(features_for_clustering)\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(x=features_tsne_2d[:, 0], y=features_tsne_2d[:, 1], hue=kmeans_labels, palette='viridis')\n",
    "plt.title('KMeans Clustering (2D t-SNE)')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x=features_tsne_2d[:, 0], y=features_tsne_2d[:, 1], hue=hierarchical_labels, palette='viridis')\n",
    "plt.title('Hierarchical Clustering (2D t-SNE)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a labels array kmeans_labels with labels for various files in file_names array. Now i want to change the labels in kmeans_values(integer values) corresponding to file names starting with \n",
    "'Jana-Gana' as 6,\n",
    "'Michael-Jackson' as 7,\n",
    "'Asha-Bhosle' as 8,\n",
    "'Kishore-Kumar' as 8,\n",
    "'Bhavgeet' as 9,\n",
    "'Lavni' as 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3D t-SNE Visualization\n",
    "tsne_3d = TSNE(n_components=3, random_state=42)\n",
    "features_tsne_3d = tsne_3d.fit_transform(features_for_clustering)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.scatter(features_tsne_3d[:, 0], features_tsne_3d[:, 1], features_tsne_3d[:, 2], c=kmeans_labels, cmap='viridis')\n",
    "ax1.set_title('KMeans Clustering (3D t-SNE)')\n",
    "\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "ax2.scatter(features_tsne_3d[:, 0], features_tsne_3d[:, 1], features_tsne_3d[:, 2], c=hierarchical_labels, cmap='viridis')\n",
    "ax2.set_title('Hierarchical Clustering (3D t-SNE)')\n",
    "\n",
    "# ax3 = fig.add_subplot(133, projection='3d')\n",
    "# ax3.scatter(features_tsne_3d[:, 0], features_tsne_3d[:, 1], features_tsne_3d[:, 2], c=svc_labels, cmap='viridis')\n",
    "# ax3.set_title('SVC-based Clustering (3D t-SNE)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
