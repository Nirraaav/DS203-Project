{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import librosa\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Asha-Bhosle-1-MFCC.csv\n",
      "Processed 1 of 495\n",
      "Processed Asha-Bhosle-1.csv\n",
      "Processed 2 of 495\n",
      "Processed Asha-Bhosle-10-MFCC.csv\n",
      "Processed 3 of 495\n",
      "Processed Asha-Bhosle-10.csv\n",
      "Processed 4 of 495\n",
      "Processed Asha-Bhosle-11-MFCC.csv\n",
      "Processed 5 of 495\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m mfcc_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     50\u001b[0m mfcc_data \u001b[38;5;241m=\u001b[39m mfcc_data[:\u001b[38;5;241m20\u001b[39m]\n\u001b[1;32m---> 51\u001b[0m mfcc_data2 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Compute aggregated MFCC features\u001b[39;00m\n\u001b[0;32m     53\u001b[0m aggregated_features \u001b[38;5;241m=\u001b[39m aggregate_mfcc_selective(mfcc_data)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "INDEX = 13\n",
    "DATA_DIR = './new-test-data/test-mfcc-v2' #update with where you stored the file\n",
    "DATA2_DIR = './new-test-data/test-mfcc-v2-copy'\n",
    "def aggregate_mfcc_selective(mfcc_data):\n",
    "    mfcc_selected = mfcc_data[INDEX:, :]\n",
    "    #features for data from 13-20\n",
    "    mfcc_mean = np.mean(mfcc_selected, axis=1)\n",
    "    mfcc_std = np.std(mfcc_selected, axis=1)\n",
    "    mfcc_max = np.max(mfcc_selected, axis=1)\n",
    "    mfcc_min = np.min(mfcc_selected, axis=1)\n",
    "    \n",
    "    features = np.concatenate([mfcc_mean, mfcc_std, mfcc_max, mfcc_min])\n",
    "    return features\n",
    "\n",
    "def aggregate_mfcc_selective2(mfcc_data):\n",
    "    mfcc_selected = mfcc_data[:3, :] # Select first 20 MFCCs\n",
    "    \n",
    "    mfcc_mean = np.mean(mfcc_selected, axis=1)\n",
    "    mfcc_std = np.std(mfcc_selected, axis=1)\n",
    "    mfcc_max = np.max(mfcc_selected, axis=1)\n",
    "    mfcc_min = np.min(mfcc_selected, axis=1)\n",
    "    \n",
    "    features = np.concatenate([mfcc_mean, mfcc_std, mfcc_max, mfcc_min])\n",
    "    return features\n",
    "\n",
    "# total number of files in the directory\n",
    "total_files = len(os.listdir(DATA_DIR))\n",
    "index = 1\n",
    "\n",
    "# Initialize lists to store features, file names, and labels\n",
    "mfcc_all_songs = []\n",
    "file_names = []\n",
    "generated_features = []\n",
    "labels = []\n",
    "\n",
    "total_songs = len(os.listdir(DATA_DIR))\n",
    "idx = 1\n",
    "\n",
    "# Iterate over all files in the specified directory\n",
    "for file_name in os.listdir(DATA_DIR):\n",
    "        file_path = os.path.join(DATA_DIR, file_name)\n",
    "        file_path2 = os.path.join(DATA2_DIR, file_name)\n",
    "        mfcc_data = pd.read_csv(file_path, header=None).values\n",
    "        mfcc_data = mfcc_data[:20]\n",
    "        mfcc_data2 = pd.read_csv(file_path2, header=None).values\n",
    "        # Compute aggregated MFCC features\n",
    "        aggregated_features = aggregate_mfcc_selective(mfcc_data)\n",
    "        aggregated_features2 = aggregate_mfcc_selective2(mfcc_data2)\n",
    "\n",
    "        skewness = skew(mfcc_data, axis=1)\n",
    "        kurt = kurtosis(mfcc_data, axis=1)\n",
    "        range_max_min = np.ptp(mfcc_data, axis=1)\n",
    "\n",
    "        total_energy = np.sum(mfcc_data ** 2, axis=1)\n",
    "        energy_entropy = -np.sum(mfcc_data ** 2 * np.log(mfcc_data ** 2 + 1e-10), axis=1)\n",
    "        q25 = np.percentile(mfcc_data, 25, axis=1)\n",
    "        q75 = np.percentile(mfcc_data, 75, axis=1)\n",
    "    \n",
    "        delta_mfcc = librosa.feature.delta(mfcc_data, order=1)\n",
    "        delta_delta_mfcc = librosa.feature.delta(mfcc_data, order=2)\n",
    "        \n",
    "\n",
    "        delta_mean = np.mean(delta_mfcc, axis=1)\n",
    "        delta_std = np.std(delta_mfcc, axis=1)\n",
    "        delta_max = np.max(delta_mfcc, axis=1)\n",
    "        delta_min = np.min(delta_mfcc, axis=1)\n",
    "        delta_skew = skew(librosa.feature.delta(mfcc_data), axis=1)\n",
    "        delta_kurtosis = kurtosis(librosa.feature.delta(mfcc_data), axis=1)\n",
    "        delta_range = np.ptp(librosa.feature.delta(mfcc_data), axis=1)\n",
    "        delta_total_energy = np.sum(librosa.feature.delta(mfcc_data) ** 2, axis=1)\n",
    "        delta_energy_entropy = -np.sum(librosa.feature.delta(mfcc_data) ** 2 * np.log(librosa.feature.delta(mfcc_data) ** 2 + 1e-10), axis=1)\n",
    "        delta_q25 = np.percentile(delta_mfcc, 25, axis=1)\n",
    "        delta_q75 = np.percentile(delta_mfcc, 75, axis=1)\n",
    "            \n",
    "        delta_delta_mean = np.mean(delta_delta_mfcc, axis=1)\n",
    "        delta_delta_std = np.std(delta_delta_mfcc, axis=1)\n",
    "        delta_delta_max = np.max(delta_delta_mfcc, axis=1)\n",
    "        delta_delta_min = np.min(delta_delta_mfcc, axis=1)\n",
    "        delta_delta_skew = skew(librosa.feature.delta(delta_delta_mfcc, order=2), axis=1)\n",
    "        delta_delta_kurtosis = kurtosis(librosa.feature.delta(delta_delta_mfcc, order=2), axis=1)\n",
    "        delta_delta_range = np.ptp(librosa.feature.delta(delta_delta_mfcc, order=2), axis=1)\n",
    "        delta_delta_total_energy = np.sum(librosa.feature.delta(delta_delta_mfcc, order=2) ** 2, axis=1)\n",
    "        delta_delta_energy_entropy = -np.sum(librosa.feature.delta(delta_delta_mfcc, order=2) \n",
    "                                              ** 2 * np.log(librosa.feature.delta(delta_delta_mfcc, order=2) ** 2 + 1e-10), axis=1)\n",
    "        delta_delta_q25 = np.percentile(delta_delta_mfcc, 25, axis=1)\n",
    "        delta_delta_q75 = np.percentile(delta_delta_mfcc, 75, axis=1)\n",
    "\n",
    "    \n",
    "        delta_mfcc2 = librosa.feature.delta(mfcc_data2, order=1)\n",
    "        delta_delta_mfcc2 = librosa.feature.delta(mfcc_data2, order=2)\n",
    "        delta_mean2 = np.mean(delta_mfcc2, axis=1)\n",
    "        delta_std2 = np.std(delta_mfcc2, axis=1)\n",
    "        delta_max2 = np.max(delta_mfcc2, axis=1)\n",
    "        delta_min2 = np.min(delta_mfcc2, axis=1)\n",
    "        delta_delta_mean2 = np.mean(delta_delta_mfcc2, axis=1)\n",
    "        delta_delta_std2 = np.std(delta_delta_mfcc2, axis=1)\n",
    "        delta_delta_max2 = np.max(delta_delta_mfcc2, axis=1)\n",
    "        delta_delta_min2 = np.min(delta_delta_mfcc2, axis=1)\n",
    "        delta_delta_range2 = np.ptp(delta_delta_mfcc2, axis=1)  \n",
    "        delta_delta_total_energy2 = np.sum(librosa.feature.delta(delta_delta_mfcc2, order=2) ** 2, axis=1)\n",
    "        delta_delta_energy_entropy2 = -np.sum(librosa.feature.delta(delta_delta_mfcc2, order=2) \n",
    "                                              ** 2 * np.log(librosa.feature.delta(delta_delta_mfcc2, order=2) ** 2 + 1e-10), axis=1)\n",
    "        delta_delta_q252 = np.percentile(delta_delta_mfcc2, 25, axis=1)\n",
    "        delta_delta_q752 = np.percentile(delta_delta_mfcc2, 75, axis=1)\n",
    "\n",
    "        # PCA_mfcc = PCA(n_components=5)\n",
    "        # PCA_mfcc.fit(mfcc_data.T)\n",
    "        # PCA_mfcc_features = PCA_mfcc.components_.flatten()\n",
    "\n",
    "        # Compile all features into a single vector\n",
    "        features = np.concatenate([\n",
    "            aggregated_features.flatten(),\n",
    "            aggregated_features2.flatten(),\n",
    "            range_max_min.flatten(),\n",
    "            skewness.flatten(),\n",
    "            kurt.flatten(),\n",
    "            total_energy.flatten(),\n",
    "            energy_entropy.flatten(),\n",
    "            q25.flatten(),\n",
    "            q75.flatten(),\n",
    "            \n",
    "            delta_mean.flatten(),\n",
    "            delta_std.flatten(),\n",
    "            delta_max.flatten(),\n",
    "            delta_min.flatten(),\n",
    "            delta_skew.flatten(),\n",
    "            delta_kurtosis.flatten(),\n",
    "            delta_range.flatten(),\n",
    "            delta_total_energy.flatten(),\n",
    "            delta_energy_entropy.flatten(),\n",
    "            delta_q25.flatten(),\n",
    "            delta_q75.flatten(),\n",
    "            \n",
    "            delta_delta_mean.flatten(),\n",
    "            delta_delta_std.flatten(),\n",
    "            delta_delta_max.flatten(),\n",
    "            delta_delta_min.flatten(),\n",
    "            delta_delta_skew.flatten(),\n",
    "            delta_delta_kurtosis.flatten(),\n",
    "            delta_delta_range.flatten(),\n",
    "            delta_delta_total_energy.flatten(),\n",
    "            delta_delta_energy_entropy.flatten(),\n",
    "            delta_delta_q25.flatten(),\n",
    "            delta_delta_q75.flatten(),\n",
    "            \n",
    "            delta_mean2.flatten(),\n",
    "            delta_std2.flatten(),\n",
    "            delta_max2.flatten(),\n",
    "            delta_min2.flatten(),\n",
    "            delta_delta_mean2.flatten(),\n",
    "            delta_delta_std2.flatten(),\n",
    "            delta_delta_max2.flatten(),\n",
    "            delta_delta_min2.flatten(),\n",
    "            delta_delta_total_energy2.flatten(),\n",
    "            delta_delta_energy_entropy2.flatten(),\n",
    "            delta_delta_q252.flatten(),\n",
    "            delta_delta_q752.flatten(),\n",
    "\n",
    "            # PCA_mfcc_features.flatten(),\n",
    "        ])\n",
    "\n",
    "        # Determine label based on filename pattern\n",
    "        if file_name.startswith('Asha-Bhosle'):\n",
    "            label = 'Asha Bhosle'\n",
    "        elif file_name.startswith('Kishore-Kumar'):\n",
    "            label = 'Kishore Kumar'\n",
    "        elif file_name.startswith('Lavni'):\n",
    "            label = 'Lavni'\n",
    "        elif file_name.startswith('Michael-Jackson'):\n",
    "            label = 'Michael Jackson'\n",
    "        elif file_name.startswith('Jana-Gana-Mana'):\n",
    "            label = 'Jana-Gana-Mana'\n",
    "        elif file_name.startswith('Bhavgeet'):\n",
    "             label = 'Bhavgeet'\n",
    "        else:\n",
    "            print(f'Unknown label for file: {file_name}')\n",
    "            \n",
    "        print(f'Processed {file_name}')\n",
    "        print(f'Processed {idx} of {total_songs}')\n",
    "        generated_features.append(features)\n",
    "        file_names.append(file_name)\n",
    "        labels.append(label)\n",
    "        idx += 1\n",
    "\n",
    "# Create DataFrame and save to CSV\n",
    "generated_features = np.vstack(generated_features)\n",
    "total_features = generated_features.shape[1]\n",
    "feature_columns = [f'feature_{i}' for i in range(total_features)]\n",
    "\n",
    "total_data_df = pd.DataFrame(generated_features, columns=feature_columns)\n",
    "total_data_df.insert(0, 'File', file_names)\n",
    "total_data_df.insert(1, 'Label', labels)\n",
    "\n",
    "# Save generated features to 'features_generated.csv'\n",
    "total_data_df.to_csv(f'features_generated_{INDEX}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_df = pd.read_csv(\"features_generated_13.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting the data into features (X) and labels (y)\n",
    "X = total_data_df.drop(columns=['File', 'Label'])\n",
    "y = total_data_df['Label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (495, 658)\n",
      "\n",
      "Columns in dataset:\n",
      " Index(['File', 'Label', 'feature_0', 'feature_1', 'feature_2', 'feature_3',\n",
      "       'feature_4', 'feature_5', 'feature_6', 'feature_7',\n",
      "       ...\n",
      "       'feature_646', 'feature_647', 'feature_648', 'feature_649',\n",
      "       'feature_650', 'feature_651', 'feature_652', 'feature_653',\n",
      "       'feature_654', 'feature_655'],\n",
      "      dtype='object', length=658)\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "                       File         Label  feature_0  feature_1  feature_2  \\\n",
      "0   Asha-Bhosle-1-MFCC.csv  Asha Bhosale  -1.915812   3.200653  -3.524275   \n",
      "1        Asha-Bhosle-1.csv  Asha Bhosale  -8.527283  -5.308342  -5.192457   \n",
      "2  Asha-Bhosle-10-MFCC.csv  Asha Bhosale  -0.200634   0.770156  -7.181722   \n",
      "3       Asha-Bhosle-10.csv  Asha Bhosale  -5.875371  -7.705338  -1.813207   \n",
      "4  Asha-Bhosle-11-MFCC.csv  Asha Bhosale  -5.700858  -2.335944  -0.169053   \n",
      "\n",
      "   feature_3  feature_4  feature_5  feature_6  feature_7  ...  feature_646  \\\n",
      "0   1.900974  -1.868573   3.710750   2.020518  10.196956  ...   198.874900   \n",
      "1  -5.387604  -3.885927  -4.809628  -7.892533  13.534690  ...   105.329232   \n",
      "2  -2.802380   2.829883  -5.803446  -9.176819  11.558065  ...    42.451782   \n",
      "3  -1.575563  -2.037466  -1.403548  -0.130934   9.330351  ...   122.780356   \n",
      "4   1.144289  -4.852632   3.813288   2.094494  11.897119  ...   124.435167   \n",
      "\n",
      "   feature_647  feature_648  feature_649  feature_650  feature_651  \\\n",
      "0  4085.596021   785.068305   832.601195    -0.896641    -0.256451   \n",
      "1  2328.006090   478.563974   499.445616    -0.529624    -0.197664   \n",
      "2  1400.830138   229.577281   205.660577    -0.549298    -0.203880   \n",
      "3  3271.640220   570.160880   556.563461    -0.611288    -0.208020   \n",
      "4  3166.141992   585.370681   591.477295    -0.516803    -0.189505   \n",
      "\n",
      "   feature_652  feature_653  feature_654  feature_655  \n",
      "0    -0.250257     0.875592     0.245655     0.251954  \n",
      "1    -0.205372     0.539639     0.195973     0.200159  \n",
      "2    -0.184268     0.620183     0.191407     0.182777  \n",
      "3    -0.208757     0.693612     0.195058     0.201558  \n",
      "4    -0.192135     0.586758     0.181156     0.195326  \n",
      "\n",
      "[5 rows x 658 columns]\n",
      "\n",
      "Data Types of each column:\n",
      " File            object\n",
      "Label           object\n",
      "feature_0      float64\n",
      "feature_1      float64\n",
      "feature_2      float64\n",
      "                ...   \n",
      "feature_651    float64\n",
      "feature_652    float64\n",
      "feature_653    float64\n",
      "feature_654    float64\n",
      "feature_655    float64\n",
      "Length: 658, dtype: object\n",
      "\n",
      "Missing values in each column:\n",
      " File           0\n",
      "Label          0\n",
      "feature_0      0\n",
      "feature_1      0\n",
      "feature_2      0\n",
      "              ..\n",
      "feature_651    0\n",
      "feature_652    0\n",
      "feature_653    0\n",
      "feature_654    0\n",
      "feature_655    0\n",
      "Length: 658, dtype: int64\n",
      "\n",
      "Statistical Summary:\n",
      "         feature_0   feature_1   feature_2   feature_3   feature_4   feature_5  \\\n",
      "count  495.000000  495.000000  495.000000  495.000000  495.000000  495.000000   \n",
      "mean    -3.934730   -3.171633   -3.176717   -2.831550   -2.260723   -1.591201   \n",
      "std      4.510940    4.531531    4.424569    4.181256    4.186246    3.926331   \n",
      "min    -16.778638  -16.783382  -17.281414  -13.844731  -14.201208  -12.282071   \n",
      "25%     -6.615744   -6.362751   -5.818090   -5.888160   -5.109298   -4.362350   \n",
      "50%     -3.701934   -3.331875   -3.144141   -3.057128   -2.499270   -1.456091   \n",
      "75%     -0.922591    0.110503   -0.327898    0.120847    0.201069    1.091155   \n",
      "max     14.736119    9.363676    9.682833    9.595983   13.470824   13.270560   \n",
      "\n",
      "        feature_6   feature_7   feature_8   feature_9  ...  feature_646  \\\n",
      "count  495.000000  495.000000  495.000000  495.000000  ...   495.000000   \n",
      "mean    -2.120396    9.097721    9.030220    9.075182  ...   122.391554   \n",
      "std      3.856264    1.591026    1.613445    1.734305  ...   103.999153   \n",
      "min    -13.518667    5.425082    5.426349    5.403582  ...    10.390508   \n",
      "25%     -4.575581    8.044290    8.023639    7.946433  ...    60.584816   \n",
      "50%     -2.559763    8.845341    8.789567    8.823377  ...    93.468281   \n",
      "75%      0.198561    9.767960    9.840060    9.934268  ...   137.914814   \n",
      "max     11.553724   18.584419   18.694048   16.990657  ...   858.347553   \n",
      "\n",
      "       feature_647  feature_648  feature_649  feature_650  feature_651  \\\n",
      "count   495.000000   495.000000   495.000000   495.000000   495.000000   \n",
      "mean   1936.130924   572.816532   514.900050    -0.577172    -0.230940   \n",
      "std    1180.993935   375.592226   350.366684     0.222743     0.077550   \n",
      "min   -2703.876170    48.241316    55.791792    -1.480678    -0.497769   \n",
      "25%    1140.594174   335.535292   294.700193    -0.700810    -0.266113   \n",
      "50%    1901.182686   503.573956   433.670949    -0.545776    -0.211179   \n",
      "75%    2611.302661   764.043541   625.360786    -0.398161    -0.180629   \n",
      "max    9098.352115  2713.520502  2654.764091    -0.212636    -0.101272   \n",
      "\n",
      "       feature_652  feature_653  feature_654  feature_655  \n",
      "count   495.000000   495.000000   495.000000   495.000000  \n",
      "mean     -0.224360     0.598690     0.229390     0.224121  \n",
      "std       0.072401     0.230524     0.076956     0.072426  \n",
      "min      -0.497820     0.202029     0.101889     0.124764  \n",
      "25%      -0.231286     0.405547     0.181140     0.181144  \n",
      "50%      -0.203476     0.570627     0.207506     0.202602  \n",
      "75%      -0.180649     0.729684     0.260661     0.230438  \n",
      "max      -0.119579     1.535361     0.497097     0.492704  \n",
      "\n",
      "[8 rows x 656 columns]\n",
      "\n",
      "Number of duplicate rows: 0\n",
      "\n",
      "Label distribution in 'Label':\n",
      " Label\n",
      "Asha Bhosale       97\n",
      "Bhavgeet           93\n",
      "Kishore Kumar      92\n",
      "Michael Jackson    82\n",
      "Lavni              74\n",
      "Jana-Gana-Mana     57\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Shape of dataset:\", total_data_df.shape)\n",
    "print(\"\\nColumns in dataset:\\n\", total_data_df.columns)\n",
    "print(\"\\nFirst 5 rows of the dataset:\\n\", total_data_df.head())\n",
    "print(\"\\nData Types of each column:\\n\", total_data_df.dtypes)\n",
    "print(\"\\nMissing values in each column:\\n\", total_data_df.isnull().sum())\n",
    "\n",
    "print(\"\\nStatistical Summary:\\n\", total_data_df.describe())\n",
    "print(\"\\nNumber of duplicate rows:\", total_data_df.duplicated().sum())\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(X, annot=True, cmap='coolwarm', square=True)\n",
    "# plt.title(\"Correlation Matrix\")\n",
    "# plt.show()\n",
    "label_column = 'Label'\n",
    "print(f\"\\nLabel distribution in '{label_column}':\\n\", total_data_df[label_column].value_counts())\n",
    "# label_column = 'Label'  # Replace 'label' with the actual name of the label column in your dataset\n",
    "# if label_column in total_data_df.columns:\n",
    "#     print(f\"\\nNumber of unique labels in '{label_column}':\", df[label_column].nunique())\n",
    "#     print(f\"\\nLabel distribution in '{label_column}':\\n\", df[label_column].value_counts())\n",
    "# else:\n",
    "#     print(f\"\\nWarning: Column '{label_column}' not found in the dataset.\")\n",
    "\n",
    "\n",
    "#Add a Correlation HeatMap\n",
    "\n",
    "#too many features to analyse the plots of all of them manually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4',\n",
       "       'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9',\n",
       "       ...\n",
       "       'feature_646', 'feature_647', 'feature_648', 'feature_649',\n",
       "       'feature_650', 'feature_651', 'feature_652', 'feature_653',\n",
       "       'feature_654', 'feature_655'],\n",
       "      dtype='object', length=656)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_columns = X.columns\n",
    "X_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_646</th>\n",
       "      <th>feature_647</th>\n",
       "      <th>feature_648</th>\n",
       "      <th>feature_649</th>\n",
       "      <th>feature_650</th>\n",
       "      <th>feature_651</th>\n",
       "      <th>feature_652</th>\n",
       "      <th>feature_653</th>\n",
       "      <th>feature_654</th>\n",
       "      <th>feature_655</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.448013</td>\n",
       "      <td>1.407633</td>\n",
       "      <td>-0.078631</td>\n",
       "      <td>1.132988</td>\n",
       "      <td>0.093770</td>\n",
       "      <td>1.351724</td>\n",
       "      <td>1.074901</td>\n",
       "      <td>0.691596</td>\n",
       "      <td>1.797615</td>\n",
       "      <td>1.051008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.736167</td>\n",
       "      <td>1.821889</td>\n",
       "      <td>0.565684</td>\n",
       "      <td>0.907685</td>\n",
       "      <td>-1.435705</td>\n",
       "      <td>-0.329291</td>\n",
       "      <td>-0.358047</td>\n",
       "      <td>1.202398</td>\n",
       "      <td>0.211573</td>\n",
       "      <td>0.384688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.019122</td>\n",
       "      <td>-0.471997</td>\n",
       "      <td>-0.456040</td>\n",
       "      <td>-0.611931</td>\n",
       "      <td>-0.388617</td>\n",
       "      <td>-0.820533</td>\n",
       "      <td>-1.498335</td>\n",
       "      <td>2.791568</td>\n",
       "      <td>2.752163</td>\n",
       "      <td>3.507031</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164228</td>\n",
       "      <td>0.332154</td>\n",
       "      <td>-0.251198</td>\n",
       "      <td>-0.044154</td>\n",
       "      <td>0.213678</td>\n",
       "      <td>0.429532</td>\n",
       "      <td>0.262529</td>\n",
       "      <td>-0.256421</td>\n",
       "      <td>-0.434673</td>\n",
       "      <td>-0.331187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.828624</td>\n",
       "      <td>0.870738</td>\n",
       "      <td>-0.906090</td>\n",
       "      <td>0.006983</td>\n",
       "      <td>1.217261</td>\n",
       "      <td>-1.073905</td>\n",
       "      <td>-1.831711</td>\n",
       "      <td>1.547952</td>\n",
       "      <td>1.742632</td>\n",
       "      <td>0.930832</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.769436</td>\n",
       "      <td>-0.453721</td>\n",
       "      <td>-0.914786</td>\n",
       "      <td>-0.883510</td>\n",
       "      <td>0.125263</td>\n",
       "      <td>0.349290</td>\n",
       "      <td>0.554310</td>\n",
       "      <td>0.093327</td>\n",
       "      <td>-0.494068</td>\n",
       "      <td>-0.571413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.430643</td>\n",
       "      <td>-1.001492</td>\n",
       "      <td>0.308480</td>\n",
       "      <td>0.300689</td>\n",
       "      <td>0.053385</td>\n",
       "      <td>0.047842</td>\n",
       "      <td>0.516426</td>\n",
       "      <td>0.146361</td>\n",
       "      <td>1.016760</td>\n",
       "      <td>0.797914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003742</td>\n",
       "      <td>1.131979</td>\n",
       "      <td>-0.007078</td>\n",
       "      <td>0.119034</td>\n",
       "      <td>-0.153322</td>\n",
       "      <td>0.295844</td>\n",
       "      <td>0.215724</td>\n",
       "      <td>0.412183</td>\n",
       "      <td>-0.446567</td>\n",
       "      <td>-0.311838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.391917</td>\n",
       "      <td>0.184603</td>\n",
       "      <td>0.680452</td>\n",
       "      <td>0.951834</td>\n",
       "      <td>-0.619775</td>\n",
       "      <td>1.377866</td>\n",
       "      <td>1.094104</td>\n",
       "      <td>1.761272</td>\n",
       "      <td>0.532733</td>\n",
       "      <td>0.160412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019670</td>\n",
       "      <td>1.042559</td>\n",
       "      <td>0.033459</td>\n",
       "      <td>0.218784</td>\n",
       "      <td>0.271296</td>\n",
       "      <td>0.534839</td>\n",
       "      <td>0.445547</td>\n",
       "      <td>-0.051813</td>\n",
       "      <td>-0.627409</td>\n",
       "      <td>-0.397980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>1.473384</td>\n",
       "      <td>0.173668</td>\n",
       "      <td>2.198650</td>\n",
       "      <td>1.723188</td>\n",
       "      <td>-0.350942</td>\n",
       "      <td>1.681128</td>\n",
       "      <td>-0.986954</td>\n",
       "      <td>-1.193034</td>\n",
       "      <td>-0.720709</td>\n",
       "      <td>-0.479995</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.754207</td>\n",
       "      <td>-0.677021</td>\n",
       "      <td>-0.906676</td>\n",
       "      <td>-0.851063</td>\n",
       "      <td>0.602826</td>\n",
       "      <td>0.617026</td>\n",
       "      <td>0.561543</td>\n",
       "      <td>-0.369468</td>\n",
       "      <td>-0.699201</td>\n",
       "      <td>-0.576998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.232561</td>\n",
       "      <td>1.222822</td>\n",
       "      <td>-0.271727</td>\n",
       "      <td>1.120561</td>\n",
       "      <td>-0.142194</td>\n",
       "      <td>0.133080</td>\n",
       "      <td>0.315777</td>\n",
       "      <td>-0.816933</td>\n",
       "      <td>-1.321320</td>\n",
       "      <td>-1.439487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312242</td>\n",
       "      <td>1.286851</td>\n",
       "      <td>0.040823</td>\n",
       "      <td>-0.232075</td>\n",
       "      <td>-1.857452</td>\n",
       "      <td>-0.294655</td>\n",
       "      <td>0.204800</td>\n",
       "      <td>2.680196</td>\n",
       "      <td>0.070870</td>\n",
       "      <td>-0.232972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>1.259294</td>\n",
       "      <td>1.427658</td>\n",
       "      <td>0.690290</td>\n",
       "      <td>1.511211</td>\n",
       "      <td>0.581687</td>\n",
       "      <td>1.827445</td>\n",
       "      <td>0.859246</td>\n",
       "      <td>0.061852</td>\n",
       "      <td>-0.812865</td>\n",
       "      <td>0.329069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286241</td>\n",
       "      <td>-0.358870</td>\n",
       "      <td>0.680062</td>\n",
       "      <td>0.319983</td>\n",
       "      <td>-2.156240</td>\n",
       "      <td>-0.611275</td>\n",
       "      <td>-0.559519</td>\n",
       "      <td>1.804840</td>\n",
       "      <td>0.476167</td>\n",
       "      <td>0.159451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.380100</td>\n",
       "      <td>0.207569</td>\n",
       "      <td>0.083692</td>\n",
       "      <td>-0.114251</td>\n",
       "      <td>0.277993</td>\n",
       "      <td>0.034446</td>\n",
       "      <td>0.420737</td>\n",
       "      <td>-0.291137</td>\n",
       "      <td>-0.166063</td>\n",
       "      <td>0.357460</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.638525</td>\n",
       "      <td>0.281883</td>\n",
       "      <td>-0.762620</td>\n",
       "      <td>-0.674475</td>\n",
       "      <td>-0.551222</td>\n",
       "      <td>0.455217</td>\n",
       "      <td>0.553785</td>\n",
       "      <td>1.110108</td>\n",
       "      <td>-0.704683</td>\n",
       "      <td>-0.537376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0.677045</td>\n",
       "      <td>1.282846</td>\n",
       "      <td>0.335784</td>\n",
       "      <td>1.272739</td>\n",
       "      <td>0.280649</td>\n",
       "      <td>1.330060</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>-0.247758</td>\n",
       "      <td>-0.540503</td>\n",
       "      <td>-0.780211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238840</td>\n",
       "      <td>-0.836190</td>\n",
       "      <td>1.009496</td>\n",
       "      <td>0.352666</td>\n",
       "      <td>-2.147172</td>\n",
       "      <td>-1.028646</td>\n",
       "      <td>-0.514455</td>\n",
       "      <td>1.839973</td>\n",
       "      <td>0.968181</td>\n",
       "      <td>0.303539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>495 rows × 656 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0     0.448013   1.407633  -0.078631   1.132988   0.093770   1.351724   \n",
       "1    -1.019122  -0.471997  -0.456040  -0.611931  -0.388617  -0.820533   \n",
       "2     0.828624   0.870738  -0.906090   0.006983   1.217261  -1.073905   \n",
       "3    -0.430643  -1.001492   0.308480   0.300689   0.053385   0.047842   \n",
       "4    -0.391917   0.184603   0.680452   0.951834  -0.619775   1.377866   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "490   1.473384   0.173668   2.198650   1.723188  -0.350942   1.681128   \n",
       "491   0.232561   1.222822  -0.271727   1.120561  -0.142194   0.133080   \n",
       "492   1.259294   1.427658   0.690290   1.511211   0.581687   1.827445   \n",
       "493   0.380100   0.207569   0.083692  -0.114251   0.277993   0.034446   \n",
       "494   0.677045   1.282846   0.335784   1.272739   0.280649   1.330060   \n",
       "\n",
       "     feature_6  feature_7  feature_8  feature_9  ...  feature_646  \\\n",
       "0     1.074901   0.691596   1.797615   1.051008  ...     0.736167   \n",
       "1    -1.498335   2.791568   2.752163   3.507031  ...    -0.164228   \n",
       "2    -1.831711   1.547952   1.742632   0.930832  ...    -0.769436   \n",
       "3     0.516426   0.146361   1.016760   0.797914  ...     0.003742   \n",
       "4     1.094104   1.761272   0.532733   0.160412  ...     0.019670   \n",
       "..         ...        ...        ...        ...  ...          ...   \n",
       "490  -0.986954  -1.193034  -0.720709  -0.479995  ...    -0.754207   \n",
       "491   0.315777  -0.816933  -1.321320  -1.439487  ...    -0.312242   \n",
       "492   0.859246   0.061852  -0.812865   0.329069  ...     0.286241   \n",
       "493   0.420737  -0.291137  -0.166063   0.357460  ...    -0.638525   \n",
       "494   0.001213  -0.247758  -0.540503  -0.780211  ...     0.238840   \n",
       "\n",
       "     feature_647  feature_648  feature_649  feature_650  feature_651  \\\n",
       "0       1.821889     0.565684     0.907685    -1.435705    -0.329291   \n",
       "1       0.332154    -0.251198    -0.044154     0.213678     0.429532   \n",
       "2      -0.453721    -0.914786    -0.883510     0.125263     0.349290   \n",
       "3       1.131979    -0.007078     0.119034    -0.153322     0.295844   \n",
       "4       1.042559     0.033459     0.218784     0.271296     0.534839   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "490    -0.677021    -0.906676    -0.851063     0.602826     0.617026   \n",
       "491     1.286851     0.040823    -0.232075    -1.857452    -0.294655   \n",
       "492    -0.358870     0.680062     0.319983    -2.156240    -0.611275   \n",
       "493     0.281883    -0.762620    -0.674475    -0.551222     0.455217   \n",
       "494    -0.836190     1.009496     0.352666    -2.147172    -1.028646   \n",
       "\n",
       "     feature_652  feature_653  feature_654  feature_655  \n",
       "0      -0.358047     1.202398     0.211573     0.384688  \n",
       "1       0.262529    -0.256421    -0.434673    -0.331187  \n",
       "2       0.554310     0.093327    -0.494068    -0.571413  \n",
       "3       0.215724     0.412183    -0.446567    -0.311838  \n",
       "4       0.445547    -0.051813    -0.627409    -0.397980  \n",
       "..           ...          ...          ...          ...  \n",
       "490     0.561543    -0.369468    -0.699201    -0.576998  \n",
       "491     0.204800     2.680196     0.070870    -0.232972  \n",
       "492    -0.559519     1.804840     0.476167     0.159451  \n",
       "493     0.553785     1.110108    -0.704683    -0.537376  \n",
       "494    -0.514455     1.839973     0.968181     0.303539  \n",
       "\n",
       "[495 rows x 656 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize the features (scaling them to have mean 0 and std 1)\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(X)\n",
    "features_scaled_df = pd.DataFrame(features_scaled, columns=X_columns)\n",
    "X = features_scaled_df\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(495, 656)\n",
      "(495, 656)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Remove features with variance below a threshold\n",
    "selector = VarianceThreshold(threshold=0.1)\n",
    "print(X.shape)\n",
    "X_selected = selector.fit_transform(X)\n",
    "print(X_selected.shape)\n",
    "X_selected = pd.DataFrame(X_selected, columns=X.columns[selector.get_support()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after dropping constant/correlated features: (495, 475)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Assuming X_selected is your feature matrix (DataFrame)\n",
    "# X_selected = your_data\n",
    "\n",
    "# Check for constant features and drop them\n",
    "constant_features = X_selected.columns[X_selected.nunique() == 1]\n",
    "X_selected = X_selected.drop(columns=constant_features)\n",
    "\n",
    "# Check for highly correlated features and drop them (optional)\n",
    "corr_matrix = X_selected.corr()\n",
    "correlated_features = set()\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.95:  # Adjust the threshold as needed\n",
    "            colname = corr_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "\n",
    "X_selected = X_selected.drop(columns=correlated_features)\n",
    "print(f\"Shape after dropping constant/correlated features: {X_selected.shape}\")\n",
    "\n",
    "# Add a constant to the model (for intercept)\n",
    "X_with_const = sm.add_constant(X_selected)\n",
    "\n",
    "# Function to calculate VIF for the current dataset\n",
    "def calculate_vif(X):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['Feature'] = X.columns\n",
    "    vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    # Remove infinite or NaN VIF values\n",
    "    vif_data = vif_data[~vif_data['VIF'].isin([np.inf, -np.inf, np.nan])]\n",
    "    return vif_data\n",
    "\n",
    "# Initial VIF calculation\n",
    "vif_data = calculate_vif(X_with_const)\n",
    "\n",
    "# Plot initial VIF values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(vif_data['Feature'], vif_data['VIF'], color='skyblue')\n",
    "plt.xlabel('VIF')\n",
    "plt.title('Initial Variance Inflation Factor (VIF) for Features')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInitial VIF values for each feature:\")\n",
    "print(vif_data)\n",
    "\n",
    "# Define the threshold\n",
    "threshold = 0.2 * 10**8\n",
    "\n",
    "# Use a for loop to iterate over the features and remove those with VIF > threshold\n",
    "features_to_drop = vif_data[vif_data['VIF'] > threshold]['Feature']\n",
    "\n",
    "# Drop all features that exceed the threshold\n",
    "for feature in features_to_drop:\n",
    "    if feature in X_selected.columns:  # Check if the feature exists in X_selected\n",
    "        print(f\"Dropping feature '{feature}' with VIF: {vif_data[vif_data['Feature'] == feature]['VIF'].values[0]}\")\n",
    "        X_selected = X_selected.drop(columns=[feature])\n",
    "\n",
    "# Recompute VIF for the remaining features\n",
    "X_with_const = sm.add_constant(X_selected)\n",
    "vif_data = calculate_vif(X_with_const)\n",
    "\n",
    "# Display the final VIF values after dropping high VIF features\n",
    "print(\"\\nFinal VIF values after dropping features:\")\n",
    "print(vif_data)\n",
    "\n",
    "# Plot VIF after dropping features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(vif_data['Feature'], vif_data['VIF'], color='skyblue')\n",
    "plt.xlabel('VIF')\n",
    "plt.title('Variance Inflation Factor (VIF) for Features After Dropping High VIF Features')\n",
    "plt.show()\n",
    "\n",
    "# Final shape of the dataset after dropping high VIF features\n",
    "print(f\"\\nShape of the dataset after removing high VIF features: {X_selected.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Fit a random forest model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_selected, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "sorted_idx = importances.argsort()\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(X_selected.columns[sorted_idx], importances[sorted_idx], align='center')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance from Random Forest')\n",
    "plt.show()\n",
    "\n",
    "# Optionally, select features with importance greater than 0.005\n",
    "selected_features = X_selected.columns[importances > 0.0005]\n",
    "print(selected_features.shape)\n",
    "X_selected = X_selected.drop(columns=X_selected.columns[importances < 0.0005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Get information gain for each feature\n",
    "info_gain = mutual_info_classif(X_selected, y)\n",
    "\n",
    "# Use X_selected.columns to match the feature set size\n",
    "selected_features = X_selected.columns[info_gain > 0.05]\n",
    "\n",
    "# Print selected features\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method of feature selection 1\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "print(X.shape)\n",
    "\n",
    "lda = LDA(n_components=3)  # LDA can have a maximum of (number of classes - 1) components\n",
    "X_reduced = lda.fit_transform(X, y)  # Transformed features for the classification model\n",
    "print(X_reduced.shape)\n",
    "#X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method of feature selection 2\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "print(X.shape)\n",
    "\n",
    "# Select top k features based on chi-square score\n",
    "selector = SelectKBest(chi2, k=10)\n",
    "X_reduced = selector.fit_transform(abs(X), y)\n",
    "print(X_reduced.shape)\n",
    "#X_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method of feature selection 3\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define model\n",
    "model = RandomForestClassifier()\n",
    "# Initialize RFE and fit to data\n",
    "rfe = RFE(estimator=model, n_features_to_select=10)\n",
    "X_rfe = rfe.fit_transform(X, y)\n",
    "print(X_rfe.shape)\n",
    "\n",
    "#X_rfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert categorical target to numeric labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Now fit the Lasso model\n",
    "lasso = Lasso(alpha=0.01).fit(X, y_encoded)\n",
    "model = SelectFromModel(lasso, prefit=True)\n",
    "X_lasso = model.transform(X)\n",
    "print(X_lasso.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method of feature selection 5\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialize PCA and fit to data\n",
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X_selected)\n",
    "print(X_pca.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method of feature selection 6\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "\n",
    "# Select top k features based on mutual information score\n",
    "selector = SelectKBest(mutual_info_classif, k=10)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "print(X_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method of feature selection 7\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Get information gain for each feature\n",
    "info_gain = mutual_info_classif(X, y)\n",
    "selected_features = X.columns[info_gain > 0.05]\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting into training (70%) and test (30%) datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#making a pie chart to show weight of each label\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'labels' is a list of the labels in your dataset (replace it with your actual labels)\n",
    "labels = ['Asha Bhosale', 'Kishore Kumar', 'Lavni', 'Michael Jackson', 'Jana Gana Mana', 'Bhavgeet']  \n",
    "\n",
    "# Count the occurrences of each label in the 'labels' column using value_counts\n",
    "label_counts = y_train.value_counts()\n",
    "\n",
    "# Data for the pie chart\n",
    "labels = label_counts.index\n",
    "sizes = label_counts.values\n",
    "\n",
    "# Create a pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=['#ff9999','#66b3ff','#99ff99','#ffcc99', '#c2c2f0','#ffb3e6'])\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Title\n",
    "plt.title('Label Distribution in Dataset')\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the above pie chart we can tell that the data is imbalanced, to fix the data imbalance we can use SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "# Count the occurrences of each label in the 'labels' column using value_counts\n",
    "label_counts = y_train.value_counts()\n",
    "\n",
    "# Data for the pie chart\n",
    "labels = label_counts.index\n",
    "sizes = label_counts.values\n",
    "\n",
    "# Create a pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=['#ff9999','#66b3ff','#99ff99','#ffcc99', '#c2c2f0','#ffb3e6'])\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Title\n",
    "plt.title('Label Distribution in Dataset')\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#dimensional reduction\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Dictionary to store models and results\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    #'SVM': SVC(kernel='rbf', C=1.0, probability=True), # SVC with probability=True for probability estimates\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=50),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=10),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=100, learning_rate=0.1),\n",
    "    # 'SVC_linear': SVC(kernel='linear', probability=True),\n",
    "    # 'SVC_rbf': SVC(kernel='rbf', probability=True),\n",
    "    'RandomForest_1': RandomForestClassifier(min_samples_leaf=1),\n",
    "    'RandomForest_3': RandomForestClassifier(min_samples_leaf=3),\n",
    "    'RandomForest_5': RandomForestClassifier(min_samples_leaf=5),\n",
    "    'NeuralNetwork_1': MLPClassifier(hidden_layer_sizes=(5)),\n",
    "    'NeuralNetwork_2': MLPClassifier(hidden_layer_sizes=(5, 5)),\n",
    "    'NeuralNetwork_3': MLPClassifier(hidden_layer_sizes=(5, 5, 5)),\n",
    "    'NeuralNetwork_4': MLPClassifier(hidden_layer_sizes=(10))\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Training, prediction, and metric calculations\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    model.fit(X_train, y_train_encoded)  # Train the model\n",
    "    y_pred_train = model.predict(X_train) #Predict on train set\n",
    "    y_pred_test = model.predict(X_test)  # Predict on the test set\n",
    "    \n",
    "    # Storing metrics\n",
    "    results[model_name] = {\n",
    "        \"Accuracy_Train\": accuracy_score(y_train_encoded, y_pred_train),\n",
    "        \"Accuracy_Test\": accuracy_score(y_test_encoded, y_pred_test),\n",
    "        \"Precision\": precision_score(y_train_encoded, y_pred_train, average='weighted'),\n",
    "        \"Precision\": precision_score(y_test_encoded, y_pred_test, average='weighted'),\n",
    "        \"Recall\": recall_score(y_train_encoded, y_pred_train, average='weighted'),\n",
    "        \"Recall\": recall_score(y_test_encoded, y_pred_test, average='weighted'),\n",
    "        \"F1 Score\": f1_score(y_train_encoded, y_pred_train, average='weighted'),\n",
    "        \"F1 Score\": f1_score(y_test_encoded, y_pred_test, average='weighted'),\n",
    "        \"Confusion Matrix\": confusion_matrix(y_test_encoded, y_pred_test),\n",
    "        \"Classification Report\": classification_report(y_train_encoded, y_pred_train),\n",
    "        \"Classification Report\": classification_report(y_test_encoded, y_pred_test)\n",
    "    }\n",
    "\n",
    "    # Plotting the confusion matrix using Plotly with numbers displayed and square grid\n",
    "    confusion_mat = results[model_name][\"Confusion Matrix\"]\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=confusion_mat,\n",
    "        x=[f\"Predicted {i}\" for i in range(len(confusion_mat))],\n",
    "        y=[f\"Actual {i}\" for i in range(len(confusion_mat))],\n",
    "        colorscale=\"Blues\",\n",
    "        text=confusion_mat,  # Display numbers in each cell\n",
    "        texttemplate=\"%{text}\",  # Format to show text values\n",
    "        showscale=True\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title=f\"Confusion Matrix for {model_name}\",\n",
    "        xaxis_title=\"Predicted Label\",\n",
    "        yaxis_title=\"Actual Label\",\n",
    "        xaxis=dict(scaleanchor=\"y\", scaleratio=1)  # Ensures square cells\n",
    "    )\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display the results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\nMetrics for {model_name}:\\n\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}:\\n{value}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAll Models Accuracy:\")\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name}: {metrics['Accuracy_Train']:.4f}\", f\"{model_name}: {metrics['Accuracy_Test']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAll Models Precison:\")\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name}: {metrics['Precision']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAll Models Recall:\")\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name}: {metrics['Recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAll Models f1-Score:\")\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name}: {metrics['F1 Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show decision boundary\n",
    "#ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify the three songs of each category\n",
    "#showcase the waveform for each of the three songs found for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
